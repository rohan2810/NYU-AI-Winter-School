{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 4 (experienced): NLP Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohan2810/NYU-AI-Winter-School/blob/master/Lab_4_(experienced)_NLP_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytk7Px1Rj5PV"
      },
      "source": [
        "# Stanford Sentiment Classification Dataset\n",
        "\n",
        "In this lab, we will learn how to classify a sentence as positive or negative sentiment. We will use the [Stanford Sentiment Classification Dataset (SST-2)](https://nlp.stanford.edu/sentiment/index.html) which is part of [GLUE Benchmark](https://gluebenchmark.com/tasks), a benchmark for evaluating machine learning models on a collection of variety language understanding tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4sAkQ1pkh-X"
      },
      "source": [
        "First, let's download SST-2 from GLUE Benchmark and unzip it.\n",
        "We can use [`wget`](https://www.gnu.org/software/wget/manual/wget.html) command for downloading a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsXovJcXhOVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06003b5-f4e6-41e6-c93d-f9b094f95782"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-30 18:38:01--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7439277 (7.1M) [application/zip]\n",
            "Saving to: ‘SST-2.zip’\n",
            "\n",
            "SST-2.zip           100%[===================>]   7.09M  9.18MB/s    in 0.8s    \n",
            "\n",
            "2020-12-30 18:38:02 (9.18 MB/s) - ‘SST-2.zip’ saved [7439277/7439277]\n",
            "\n",
            "Archive:  SST-2.zip\n",
            "   creating: SST-2/\n",
            "  inflating: SST-2/dev.tsv           \n",
            "   creating: SST-2/original/\n",
            "  inflating: SST-2/original/README.txt  \n",
            "  inflating: SST-2/original/SOStr.txt  \n",
            "  inflating: SST-2/original/STree.txt  \n",
            "  inflating: SST-2/original/datasetSentences.txt  \n",
            "  inflating: SST-2/original/datasetSplit.txt  \n",
            "  inflating: SST-2/original/dictionary.txt  \n",
            "  inflating: SST-2/original/original_rt_snippets.txt  \n",
            "  inflating: SST-2/original/sentiment_labels.txt  \n",
            "  inflating: SST-2/test.tsv          \n",
            "  inflating: SST-2/train.tsv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si45myuDlLCA"
      },
      "source": [
        "We can view the content of the downloaded SST-2 folder using `ls` command.\n",
        "We can see that it contains a train, dev, and test data in `tsv` format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_xbEY19k6Eg",
        "outputId": "66b431db-63c5-4a5c-b109-391c52e60116"
      },
      "source": [
        "!ls SST-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.tsv  original  test.tsv  train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmh9JQbSlVdI"
      },
      "source": [
        "Let's explore the content of `train.tsv`. We can use the `head -n` command to quickly read the frist `n` lines of any file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6U_w2Egl0X3",
        "outputId": "794d7969-31c3-4e1e-a471-023f7b2cc5c1"
      },
      "source": [
        "!head -5 SST-2/train.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence\tlabel\n",
            "hide new secretions from the parental units \t0\n",
            "contains no wit , only labored gags \t0\n",
            "that loves its characters and communicates something rather beautiful about human nature \t1\n",
            "remains utterly satisfied to remain the same throughout \t0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNkSpORSmPKX"
      },
      "source": [
        "As we can see, the first line is the name of each column (sentence and label) and the rest of the lines are the examples of the datasets. SST-2 is a binary classification dataset: Label 0 corresponds to negative sentiment and 1 corresponds to positive sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwG_XocUhJkV"
      },
      "source": [
        "Now, we will read the `tsv` file and store the data in python.\n",
        "We will use pandas library for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Z127cdiw9u4A",
        "outputId": "67360307-96e2-435a-c1db-1d4b9f7ad703"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"SST-2/train.tsv\", sep='\\t')\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0       hide new secretions from the parental units       0\n",
              "1               contains no wit , only labored gags       0\n",
              "2  that loves its characters and communicates som...      1\n",
              "3  remains utterly satisfied to remain the same t...      0\n",
              "4  on the worst revenge-of-the-nerds clichés the ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sQ9i4a29xwa",
        "outputId": "f1cac342-ab14-4ddd-afb1-7f6f7f57023d"
      },
      "source": [
        "print(data[\"sentence\"][:2].tolist())\n",
        "print(data[\"label\"][:2].tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hide new secretions from the parental units ', 'contains no wit , only labored gags ']\n",
            "[0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzU8Rv0t45dc"
      },
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "We are going to use a simple Bag-of-Word classifier model. \n",
        "Before we can train the model, we need to prepare the input to the model.\n",
        "\n",
        "We need to define a python function to load the dataset files. \n",
        "We will then perform preprocessing steps** that involve tokenizing the sentences in the dataset into words, removing the stop words (which does not add much meaning to a sentence such as “the”, “a”, “an”, “in”) and punctuation, lower-casing all the characters.\n",
        "\n",
        "**Note that we normally do not perofrm the removal of stop word and punctuation when we use the more recent powerful state-of-the-art models, such as Google's BERT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjQCwh4-69et"
      },
      "source": [
        "First, we need to install the `spacy` library which will be used in preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh2aJFjbmtEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53805dfa-e1d2-4630-f4c3-8ef56c55d1cd"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/bf/ca7bb25edd21f1cf9d498d0023808279672a664a70585e1962617ca2740c/spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/1a/c3e4ab982214c63d743fad57c45c5e68ee49e4ea4384d27b28595a26ad26/thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Installing collected packages: thinc, spacy\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed spacy-2.3.5 thinc-7.4.5\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1MB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (51.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047109 sha256=b37861c020c142e9b4dbdaeea580ecd6a14bd45621e6ff1a65a5f52535bdba52\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3rw0lln1/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AFYNKe-hfON"
      },
      "source": [
        "We will define a `preprocess_sent` function that tokenize, lowercase, and remove punctuation and stop words from a sentence using `spacy` tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUFGgbIm7HxV",
        "outputId": "575e5df4-52a3-4ad7-e957-5bcb05f1a6f2"
      },
      "source": [
        "import string\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create a blank Tokenizer with just the English vocab\n",
        "nlp_en = English()\n",
        "tokenizer = Tokenizer(nlp_en.vocab)\n",
        "\n",
        "#get a list of punctuations in English\n",
        "punctuations = string.punctuation\n",
        "print(punctuations)\n",
        "\n",
        "# tokenize, lowercase, and remove punctuation and stop words\n",
        "def preprocess_sent(sent):\n",
        "  tokens = tokenizer(sent)\n",
        "  # for each token in tokens, lower-case token's text if it's not in punctuations list and STOP_WORDS list\n",
        "  return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in STOP_WORDS)]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnoa2QKNEhIP"
      },
      "source": [
        "Let's try to preprocess a sentence using `preprocess_sent` function we defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOqGMKo_Dek8",
        "outputId": "20f3d085-aa68-428e-9780-89c521c85f77"
      },
      "source": [
        "# Example\n",
        "tokens = preprocess_sent(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'looking', 'buying', 'u.k.', 'startup', '$1', 'billion']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Sqps5oh9gj"
      },
      "source": [
        "Now, we will define a function that reads `tsv` files from the data paths and return a dictionary of datasets.\n",
        "\n",
        "The returned dictionary will look like this:\n",
        "datasets = {\n",
        "\"train\" : (train_sentence_list, train_labels),\n",
        "\"dev\": (dev_sentence_list, dev_labels),\n",
        "\"test\": (test_sentence_list, None)\n",
        "\n",
        "}\n",
        "\n",
        "Note that SST-2 dataset does not contain labels in test set. Therefore, our test labels are `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8idpXEy8Lxj"
      },
      "source": [
        "import os\n",
        "def load_datasets(data_paths):\n",
        "    \"\"\"\n",
        "        for each path in data_paths:\n",
        "          load data from the tsv file path \n",
        "          pre-process each sentence and label in data\n",
        "          and store them in datasets dictionary\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    for name, path in data_paths.items():\n",
        "        print(\"Loading {} dataset from {}\".format(name, path))\n",
        "        tmp_data = pd.read_csv(os.path.join(\"SST-2\", path), sep='\\t')\n",
        "        sentences  = [preprocess_sent(sent) for sent in tmp_data['sentence']] \n",
        "        if 'label' in tmp_data:\n",
        "            labels = tmp_data['label']\n",
        "        else:\n",
        "            labels = None # Test dataset has no label\n",
        "        datasets[name] = (sentences, labels)\n",
        "    return datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcMJ7Kcfiyx6"
      },
      "source": [
        "Now, we will load the datasets using the `load_datasets` function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuEnbhba8on_",
        "outputId": "38d6edcc-1969-4c77-829f-0eaa06f7173b"
      },
      "source": [
        "data_paths = {\n",
        "                \"train\": \"train.tsv\",\n",
        "                \"dev\": \"dev.tsv\",\n",
        "                \"test\": \"test.tsv\"\n",
        "              }\n",
        "\n",
        "datasets = load_datasets(data_paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading train dataset from train.tsv\n",
            "Loading dev dataset from dev.tsv\n",
            "Loading test dataset from test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XgMJFPIFHvW"
      },
      "source": [
        "Let's check how some examples in the preprocessed train dataset look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjJ14x8uLTNb",
        "outputId": "3e33d99c-99cd-4b77-8adb-1ff0cb73e119"
      },
      "source": [
        "print (\"Train dataset size is {}\".format(len(datasets['train'][0])))\n",
        "print (\"Val dataset size is {}\".format(len(datasets['dev'][0])))\n",
        "print (\"Test dataset size is {}\".format(len(datasets['test'][0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset size is 67349\n",
            "Val dataset size is 872\n",
            "Test dataset size is 1821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKdRZLtrFGeP",
        "outputId": "b051313b-c5df-48a1-9650-53cc9ebbae7d"
      },
      "source": [
        "for ex in datasets['train'][0][:5]:\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hide', 'new', 'secretions', 'parental', 'units']\n",
            "['contains', 'wit', 'labored', 'gags']\n",
            "['loves', 'characters', 'communicates', 'beautiful', 'human', 'nature']\n",
            "['remains', 'utterly', 'satisfied', 'remain']\n",
            "['worst', 'revenge-of-the-nerds', 'clichés', 'filmmakers', 'dredge']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4QCpTsVFbrg"
      },
      "source": [
        "Now, we will build a vocabulary using the tokens (words in this case) in the training data. We will define a `token2id` and `id2token` dictionary.\n",
        "`token2id` will take a token and return the corresponding index of the token in the vocabulary. This is used to convert word tokens into numbers. `id2token` is the reverse of `token2id`.\n",
        "\n",
        "e.g. `token2id['hi'] = 3`.  `id2token[3]='hi'`\n",
        "\n",
        "\n",
        "\n",
        "First, we will define a pad token, which is used to pad the sentences to have the same length, and an unknown token for words that do not appear in dictionary.\n",
        "Then, we will define a maximum vocabulary size (`max_vocab_size`). If the total number of unique tokens in our train dataset is larger than `max_vocab_size`, we will choose the top most frequent `max_vocab_size` as tokens in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkMJHCTmLSav"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "max_vocab_size = 10000\n",
        "# save index 0 for unk and 1 for pad\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "\n",
        "def build_vocab(train_tokens):\n",
        "    # Returns:\n",
        "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
        "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
        "    all_tokens = [tokens for token_list in train_tokens for tokens in token_list]\n",
        "    token_counter = Counter(all_tokens)\n",
        "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
        "    id2token = list(vocab)\n",
        "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
        "    id2token = ['<pad>', '<unk>'] + id2token\n",
        "    token2id['<pad>'] = PAD_IDX \n",
        "    token2id['<unk>'] = UNK_IDX\n",
        "    return token2id, id2token\n",
        "\n",
        "token2id, id2token = build_vocab(datasets['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2OWPIsROT3p"
      },
      "source": [
        "Let's check the token2id and id2token by loading a random token from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp0dnEkPNs4X",
        "outputId": "6393dd02-1f15-479e-faad-fb46635ded67"
      },
      "source": [
        "import random\n",
        "random_token_id = random.randint(0, len(id2token)-1)\n",
        "random_token = id2token[random_token_id]\n",
        "\n",
        "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
        "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token id 4908 ; token k-19\n",
            "Token k-19; token id 4908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DtEROPTOdYD"
      },
      "source": [
        "Now, we will convert the tokens in our datasets into number indices based on `token2id`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TedEajIeTB_R",
        "outputId": "9aa4bd70-1c44-4bc5-b7a2-56d2b7c9ed9d"
      },
      "source": [
        "# convert token to id in the dataset\n",
        "def token2index_dataset(tokens_data):\n",
        "    indices_data = []\n",
        "    # for each token list in tokens_data, convert to indices\n",
        "    # WRITE YOUR OWN CODE\n",
        "    for tokens in tokens_data:\n",
        "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
        "        indices_data.append(index_list)\n",
        "    return indices_data\n",
        "\n",
        "train_data_indices, train_targets = token2index_dataset(datasets['train'][0]), datasets['train'][1]\n",
        "val_data_indices, val_targets = token2index_dataset(datasets['dev'][0]), datasets['dev'][1]\n",
        "test_data_indices, test_targets = token2index_dataset(datasets['test'][0]), datasets['test'][1]\n",
        "\n",
        "# double checking\n",
        "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
        "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
        "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset size is 67349\n",
            "Val dataset size is 872\n",
            "Test dataset size is 1821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mK01JrMMXsL",
        "outputId": "1c3939f1-a624-4dc7-a7f7-afb7fabf9f1b"
      },
      "source": [
        "print(datasets['train'][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        1\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "67344    1\n",
            "67345    0\n",
            "67346    1\n",
            "67347    1\n",
            "67348    0\n",
            "Name: label, Length: 67349, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNLS8dhTTJ_h",
        "outputId": "0dcd13ca-0f7f-4fd6-ecb3-adf518a9462e"
      },
      "source": [
        "print(\"Token2ID: \", train_data_indices[0])\n",
        "print(\"Recovering tokens from train_data_indices: \", [id2token[idx] for idx in train_data_indices[0]])\n",
        "print(\"original tokens in sentence: \", datasets['train'][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token2ID:  [4193, 23, 1, 6981, 8715]\n",
            "Recovering tokens from train_data_indices:  ['hide', 'new', '<unk>', 'parental', 'units']\n",
            "original tokens in sentence:  ['hide', 'new', 'secretions', 'parental', 'units']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZtL0FoSTxtI"
      },
      "source": [
        "We can see that *secretions* is replaced with `<unk>` token as it's not in the most frequent `max_vocab_size` number of tokens.\n",
        "\n",
        "Note that, unlike image data which is continuous stream of pixel values, the natural language data is discrete, meaning its values are distinct, separate, and can only take on certain values. If we randomly change the value of a token in the sentence, the meaning of the entire sentence can change entirely.  \n",
        "e.g. \"I love apple\" --> \"I hate apple\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0v3t_n-6nEc"
      },
      "source": [
        "Now, we will create a Dataset object and DataLoader object, as well as a function for batching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKKUdt1Q6wVe"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SSTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, data_list, target_list, MAX_SENTENCE_LENGTH=200):\n",
        "        \"\"\"\n",
        "        @param data_list: list of newsgroup tokens \n",
        "        @param target_list: list of newsgroup targets \n",
        "\n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "\n",
        "        if target_list is not None:\n",
        "            self.target_list = target_list\n",
        "        else:\n",
        "            # if target list is None, create a dummy target list\n",
        "            self.target_list = [0] * len(data_list)\n",
        "        self.MAX_SENTENCE_LENGTH = MAX_SENTENCE_LENGTH\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        token_idx = self.data_list[key][:self.MAX_SENTENCE_LENGTH]\n",
        "        label = self.target_list[key]\n",
        "        return [token_idx, label]\n",
        "\n",
        "\n",
        "\n",
        "def generate_batch(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all \n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    labels = torch.LongTensor([entry[1] for entry in batch])\n",
        "    data_list = []\n",
        "    # padding\n",
        "    max_len = max(len(entry[0]) for entry in batch)\n",
        "    for entry in batch:\n",
        "        padded_vec = np.pad(np.array(entry[0]),\n",
        "                            pad_width=((0,max_len-len(entry[0]))), \n",
        "                            mode=\"constant\", constant_values=token2id['<pad>'])\n",
        "        data_list.append(padded_vec)\n",
        "    return [torch.from_numpy(np.array(data_list)).long(), labels]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSPpFVfA5-EO"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = SSTDataset(train_data_indices, train_targets)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=generate_batch,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = SSTDataset(val_data_indices, val_targets)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=generate_batch,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_dataset = SSTDataset(test_data_indices, test_targets)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=generate_batch,\n",
        "                                           shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgpwGI8N4FHJ"
      },
      "source": [
        "# Word2Vec: Pre-trained Word Vectors\n",
        "\n",
        "We will initialize the embedding layer of our model with pre-trained Word2Vec vector. To do this, we will use [`gensim`](https://radimrehurek.com/gensim/index.html) library for downloading and loading word vector. `gensim` library also contains other built-in functions such as finding similarity between word vectors, topic modeling, etc.\n",
        "\n",
        "First, let's install `gensim`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQHYFGKa3XJM",
        "outputId": "8ea698e9-6a3d-459b-c1d2-94a0b6855e0d"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SoJKBNd43SR"
      },
      "source": [
        "We will now download a pretrained word vectors that are trained on Google News dataset (This can take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54eacb-G3vYC"
      },
      "source": [
        "import gensim.downloader as api\n",
        "pretrained_word_vectors = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz9sBTXn5ULi"
      },
      "source": [
        "Let's print out the vector for `student`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vbWhwad5BmE",
        "outputId": "890f1472-10be-4a55-ca6d-f55c3b256a33"
      },
      "source": [
        "pretrained_word_vectors['student']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03686523,  0.0201416 ,  0.22167969,  0.15527344,  0.17871094,\n",
              "        0.03149414,  0.31445312, -0.03369141,  0.15429688, -0.375     ,\n",
              "        0.05102539, -0.13183594, -0.11962891, -0.13867188, -0.02026367,\n",
              "        0.01318359, -0.06738281, -0.06591797, -0.02502441, -0.140625  ,\n",
              "        0.02160645,  0.17382812, -0.00177765, -0.09179688, -0.09765625,\n",
              "       -0.4921875 , -0.13671875, -0.00570679,  0.16992188,  0.10107422,\n",
              "        0.09423828, -0.10986328, -0.08496094,  0.05419922, -0.06542969,\n",
              "       -0.0168457 ,  0.11230469,  0.13964844, -0.08300781,  0.22265625,\n",
              "       -0.23828125,  0.11767578, -0.04614258,  0.0859375 ,  0.17089844,\n",
              "       -0.06884766,  0.04003906, -0.10351562,  0.15917969,  0.04956055,\n",
              "       -0.10888672, -0.15039062,  0.01507568, -0.05419922,  0.25      ,\n",
              "       -0.09521484, -0.11816406,  0.11132812,  0.20507812, -0.10009766,\n",
              "        0.0168457 , -0.09521484, -0.00308228, -0.01348877,  0.09277344,\n",
              "       -0.08447266,  0.08496094, -0.05541992,  0.15820312,  0.10546875,\n",
              "       -0.23144531,  0.08251953,  0.01470947, -0.21289062, -0.17871094,\n",
              "        0.4375    ,  0.14550781, -0.01721191,  0.0859375 , -0.00189972,\n",
              "        0.02709961, -0.13574219, -0.01806641,  0.20605469, -0.16992188,\n",
              "        0.02722168, -0.12597656,  0.30664062, -0.03051758,  0.24902344,\n",
              "       -0.11279297, -0.37109375, -0.15429688, -0.16601562, -0.11865234,\n",
              "        0.07373047,  0.1796875 ,  0.26367188, -0.02478027, -0.20214844,\n",
              "        0.03588867, -0.10546875, -0.09033203, -0.38476562, -0.09570312,\n",
              "       -0.05639648, -0.14453125, -0.16601562,  0.23925781, -0.234375  ,\n",
              "       -0.38085938, -0.16308594,  0.09960938,  0.01062012,  0.08251953,\n",
              "       -0.11230469, -0.08007812, -0.02978516,  0.14941406,  0.17480469,\n",
              "        0.06835938,  0.07470703, -0.04931641, -0.140625  ,  0.0019989 ,\n",
              "        0.08007812,  0.10400391,  0.0255127 ,  0.07714844,  0.25390625,\n",
              "       -0.05200195, -0.29882812, -0.13085938, -0.10058594, -0.20800781,\n",
              "       -0.11474609,  0.13183594,  0.04858398, -0.34570312,  0.03759766,\n",
              "       -0.35742188, -0.2109375 , -0.09130859,  0.06640625, -0.0246582 ,\n",
              "        0.23632812, -0.04956055,  0.30078125,  0.14941406,  0.04541016,\n",
              "        0.41015625,  0.03063965, -0.05712891,  0.04516602, -0.03637695,\n",
              "       -0.20898438,  0.10693359, -0.19628906,  0.15332031,  0.04174805,\n",
              "       -0.29101562,  0.01403809,  0.12060547,  0.234375  , -0.04711914,\n",
              "        0.18554688,  0.17773438, -0.16113281,  0.12988281, -0.03149414,\n",
              "       -0.04711914, -0.0300293 ,  0.07128906, -0.15917969,  0.10107422,\n",
              "        0.046875  ,  0.17578125, -0.00115967, -0.03320312,  0.15234375,\n",
              "        0.12109375, -0.14453125,  0.15234375, -0.16503906,  0.17089844,\n",
              "        0.05761719,  0.06054688,  0.01556396,  0.02636719, -0.12792969,\n",
              "        0.14550781, -0.03540039, -0.06835938,  0.44335938, -0.43945312,\n",
              "       -0.09082031,  0.16796875, -0.20703125,  0.03112793, -0.03759766,\n",
              "       -0.17089844,  0.21289062,  0.09863281, -0.22265625, -0.23339844,\n",
              "       -0.35742188, -0.03112793, -0.16894531, -0.09082031, -0.15136719,\n",
              "        0.21484375, -0.12109375,  0.24511719, -0.37304688, -0.26171875,\n",
              "        0.12060547, -0.09667969,  0.02636719, -0.078125  , -0.05371094,\n",
              "       -0.01330566, -0.28515625,  0.04760742, -0.04418945,  0.07958984,\n",
              "        0.23828125,  0.02319336, -0.05957031,  0.18554688, -0.04150391,\n",
              "        0.2109375 ,  0.1328125 , -0.00668335,  0.16601562, -0.08007812,\n",
              "       -0.12109375,  0.02978516, -0.06787109, -0.00665283,  0.3046875 ,\n",
              "        0.390625  , -0.15234375, -0.08056641,  0.04052734, -0.1328125 ,\n",
              "       -0.07275391, -0.32226562,  0.01623535, -0.18847656,  0.17675781,\n",
              "       -0.04296875,  0.01202393,  0.05126953,  0.16601562, -0.10986328,\n",
              "       -0.21484375,  0.04394531,  0.12158203, -0.05273438,  0.16308594,\n",
              "        0.0625    ,  0.02294922, -0.18652344, -0.31445312, -0.01196289,\n",
              "        0.02319336, -0.01831055,  0.19140625, -0.20507812, -0.00750732,\n",
              "        0.09912109, -0.09423828,  0.06225586,  0.22070312, -0.15722656,\n",
              "       -0.17089844,  0.01696777, -0.29296875, -0.07324219, -0.18847656,\n",
              "        0.00811768, -0.04931641, -0.11279297,  0.19238281, -0.13183594,\n",
              "        0.10449219, -0.18066406,  0.11865234,  0.34375   ,  0.24414062,\n",
              "       -0.15136719, -0.12695312, -0.20214844, -0.09179688,  0.05786133,\n",
              "        0.171875  ,  0.03833008,  0.11474609,  0.09814453,  0.08642578],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy8f0IAM5o7Q"
      },
      "source": [
        "Let's use the built-in `most_similar` function to print out the top 5 words that are most similar to car."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5M39lsj5g7B",
        "outputId": "8b5d1b4a-cbaa-4de3-c711-af762e3129df"
      },
      "source": [
        "print(pretrained_word_vectors.most_similar(positive=['car'], topn=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962820053101), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRNpZtH_mYv"
      },
      "source": [
        "Using `most_similar` function of gensim, explore a few words of your choice and their most similar words returned by gensim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxFHYWdn_4Tm"
      },
      "source": [
        "#WRITE YOUR OWN CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "putdSc-R-i2J"
      },
      "source": [
        "Now, we will create a `word2vec_vectors` embedding which has the same vobulary indices as `token2id` using `pretrained_word_vectors`. This `word2vec_vectors` will be used to create the Embedding layer of our BagOfWord model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfWqK-_v5yM0"
      },
      "source": [
        "import torch\n",
        "word2vec_vectors = []\n",
        "W2V_SIZE = 300\n",
        "for token, idx in token2id.items():\n",
        "    if token in pretrained_word_vectors:\n",
        "        word2vec_vectors.append(torch.FloatTensor(pretrained_word_vectors[token]))\n",
        "    else:\n",
        "        word2vec_vectors.append(torch.rand(W2V_SIZE))\n",
        "word2vec_vectors=torch.stack(word2vec_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Gbf3PV9tth",
        "outputId": "a7f8f710-f111-42e4-f019-9985aa24d188"
      },
      "source": [
        "print(\"size of word2vec_vectors: \", word2vec_vectors.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of word2vec_vectors:  torch.Size([10002, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0Nae64-ffm"
      },
      "source": [
        "# Model\n",
        "\n",
        "Now we will build a class for a simple BagOfWord model.\n",
        "It consists of:\n",
        "- an embedding layer\n",
        "- a linear layer.\n",
        "\n",
        "BagOfWord model uses the average of word embeddings of each word in the sentence as the sentence representation. and forward this representation as the input to a linear classifier layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrUO8UdX-Ztd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextBoW(nn.Module):\n",
        "    def __init__(self, pre_trained_emb, num_class):\n",
        "        super(TextBoW, self).__init__()\n",
        "        # load pre-trained embeddings to Embedding layer\n",
        "        self.embedding = nn.Embedding.from_pretrained(pre_trained_emb)\n",
        "        self.embedding.weight.requires_grad=True\n",
        "        embed_dim = pre_trained_emb.size(1)\n",
        "        # a fully-connected linear layer for classification\n",
        "        self.fc = nn.Linear(embed_dim, num_class) # write your own code\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # initialize weights for fc layer\n",
        "        initrange = 0.05\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #WRITE YOUR OWN CODE\n",
        "        # forward input x into embedding layer\n",
        "        # average the output of embedding layer and forward it to linear fc layer\n",
        "        x = self.embedding(x)\n",
        "        x = torch.mean(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK-gDDGe-nKf"
      },
      "source": [
        "We will initialize a model using `TextBoW` model class we just created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6bzsB5b-kFd"
      },
      "source": [
        "EMBED_DIM = 300\n",
        "VOCAB_SIZE = len(token2id)\n",
        "NUN_CLASS = 2 # we have 2 label classes (pos and neg)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# Train on GPU device if there's cuda (GPU), else use CPU device\n",
        "\n",
        "model = TextBoW(word2vec_vectors, NUN_CLASS).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnwYJocq_R_4"
      },
      "source": [
        "Let's check how many learnable parameters our TextBoW model has."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAZpP8yN_JdR",
        "outputId": "e707f466-570a-4f79-e99b-277cf92c3d83"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,001,202 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MO7lNKL_W_E"
      },
      "source": [
        "Now we will create functions for training and evaluation of our model.\n",
        "\n",
        "The train_func will:\n",
        "- loop through the batches in training data_loader\n",
        "- forward each batch through the model\n",
        "- compute the loss and accuracy of model after each forward step\n",
        "- backpropagate the loss and update the model\n",
        "- compute the accuracy  and total loss to keep track of the performance throughout the training.\n",
        "\n",
        "The eval will:\n",
        "- loop through the batches in an input data_loader\n",
        "- forward each batch through the model\n",
        "- compute the loss and accuracy of model after each forward step\n",
        "- compute the accuracy  and total loss to keep track of the performance throughout the training if it's not in test mode. (Note that we do not have test labels so we can't compute loss and accuracy on test dataset). If it's in test_mode, we will keep track of model's predictions instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAOoJ0D-_aiy"
      },
      "source": [
        "def train_func(model, train_loader, optimizer, criterion):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    num_examples = 0\n",
        "    model.train()\n",
        "\n",
        "    # for each batch in train data loader\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        input_text, labels = batch[0], batch[1]\n",
        "\n",
        "        # WRITE YOUR OWN CODE\n",
        "        # clear optimizer gradient\n",
        "        # forward input_text through model\n",
        "        # compute loss\n",
        "        # backpropagate loss\n",
        "        # take a step with optimizer\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_text)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute total loss and accuracy\n",
        "        train_loss += loss.item()\n",
        "        train_acc += (output.argmax(1) == labels).sum().item()\n",
        "        num_examples += labels.size(0)\n",
        "        \n",
        "    return train_loss / num_examples, train_acc / num_examples\n",
        "\n",
        "\n",
        "def eval(data_loader, test_mode=False):\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "    num_examples = 0\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    for batch in data_loader:\n",
        "        input_text, labels = batch[0], batch[1]\n",
        "\n",
        "        # As we are not doing training, \n",
        "        # we don't need to keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            output = model(input_text)\n",
        "\n",
        "            if not test_mode:\n",
        "                loss = criterion(output, labels)\n",
        "                eval_loss += loss.item()\n",
        "                eval_acc += (output.argmax(1) == labels).sum().item()\n",
        "                num_examples += labels.size(0)\n",
        "            else:\n",
        "                predictions.extend(output.argmax(1).tolist())\n",
        "                \n",
        "    if test_mode:\n",
        "        return predictions\n",
        "    return eval_loss / num_examples, eval_acc / num_examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfCwxzTCdF4k"
      },
      "source": [
        "# Time to train!\n",
        "\n",
        "Now, we can start training our model.\n",
        "\n",
        "First, we will define:\n",
        "- the number of epochs (N_EPOCHS) which is the number of times we will loop through the training dataset using `train_func`\n",
        "- criterion for computing the loss function (In our case, CrossEntropyLoss for classification)\n",
        "- an optimizer and learning rate: We use the simple SGD optimizer with learning rate 0.5.\n",
        "\n",
        "We will then train the models for `N_EPOCHS`.\n",
        "For each epoch, \n",
        "- we will run the `train_func` to train the model\n",
        "- we will evaluate the model on validation dataset using `eval` function and save the model checkpoint with the best validation score. (Validation dataset is used to tune or select the best model. Note that you should not use the test dataset to tune the model. It will produce a biased model overfitted to the test data.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs4uLBom_pEe",
        "outputId": "d7700e2e-aac5-4593-f905-824ed5e1e74a"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "best_val_acc = 0\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # train model on train_loader\n",
        "    train_loss, train_acc = train_func(model, train_loader, optimizer, criterion)\n",
        "    # Evaluate the performance on validation dataset\n",
        "    valid_loss, valid_acc = eval(val_loader)\n",
        "\n",
        "    # If current validation_acc is better than best_val_acc,\n",
        "    # update best_val_acc and save model\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        torch.save(model.state_dict(), './saved_model')\n",
        "\n",
        "    # this is just to compute how long the whole process take for an epoch\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    # Print out the loss and accuracy for this epoch\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0200(train)\t|\tAcc: 63.8%(train)\n",
            "\tLoss: 0.0011(valid)\t|\tAcc: 60.6%(valid)\n",
            "Epoch: 2  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0195(train)\t|\tAcc: 65.6%(train)\n",
            "\tLoss: 0.0015(valid)\t|\tAcc: 69.2%(valid)\n",
            "Epoch: 3  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0191(train)\t|\tAcc: 66.7%(train)\n",
            "\tLoss: 0.0010(valid)\t|\tAcc: 65.1%(valid)\n",
            "Epoch: 4  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0188(train)\t|\tAcc: 67.8%(train)\n",
            "\tLoss: 0.0010(valid)\t|\tAcc: 61.9%(valid)\n",
            "Epoch: 5  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0184(train)\t|\tAcc: 68.9%(train)\n",
            "\tLoss: 0.0018(valid)\t|\tAcc: 67.8%(valid)\n",
            "Epoch: 6  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0181(train)\t|\tAcc: 69.7%(train)\n",
            "\tLoss: 0.0015(valid)\t|\tAcc: 63.8%(valid)\n",
            "Epoch: 7  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0178(train)\t|\tAcc: 70.4%(train)\n",
            "\tLoss: 0.0006(valid)\t|\tAcc: 75.8%(valid)\n",
            "Epoch: 8  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0177(train)\t|\tAcc: 70.5%(train)\n",
            "\tLoss: 0.0007(valid)\t|\tAcc: 77.9%(valid)\n",
            "Epoch: 9  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0173(train)\t|\tAcc: 71.7%(train)\n",
            "\tLoss: 0.0010(valid)\t|\tAcc: 73.4%(valid)\n",
            "Epoch: 10  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0171(train)\t|\tAcc: 72.0%(train)\n",
            "\tLoss: 0.0010(valid)\t|\tAcc: 77.9%(valid)\n",
            "Epoch: 11  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0169(train)\t|\tAcc: 72.7%(train)\n",
            "\tLoss: 0.0011(valid)\t|\tAcc: 75.9%(valid)\n",
            "Epoch: 12  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0167(train)\t|\tAcc: 73.1%(train)\n",
            "\tLoss: 0.0016(valid)\t|\tAcc: 61.5%(valid)\n",
            "Epoch: 13  | time in 0 minutes, 31 seconds\n",
            "\tLoss: 0.0164(train)\t|\tAcc: 73.7%(train)\n",
            "\tLoss: 0.0017(valid)\t|\tAcc: 62.2%(valid)\n",
            "Epoch: 14  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0163(train)\t|\tAcc: 73.8%(train)\n",
            "\tLoss: 0.0015(valid)\t|\tAcc: 59.7%(valid)\n",
            "Epoch: 15  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0161(train)\t|\tAcc: 74.4%(train)\n",
            "\tLoss: 0.0016(valid)\t|\tAcc: 66.6%(valid)\n",
            "Epoch: 16  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0160(train)\t|\tAcc: 74.5%(train)\n",
            "\tLoss: 0.0016(valid)\t|\tAcc: 67.9%(valid)\n",
            "Epoch: 17  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0159(train)\t|\tAcc: 74.8%(train)\n",
            "\tLoss: 0.0012(valid)\t|\tAcc: 77.9%(valid)\n",
            "Epoch: 18  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0157(train)\t|\tAcc: 75.2%(train)\n",
            "\tLoss: 0.0014(valid)\t|\tAcc: 77.3%(valid)\n",
            "Epoch: 19  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0155(train)\t|\tAcc: 75.7%(train)\n",
            "\tLoss: 0.0022(valid)\t|\tAcc: 75.3%(valid)\n",
            "Epoch: 20  | time in 0 minutes, 32 seconds\n",
            "\tLoss: 0.0155(train)\t|\tAcc: 75.8%(train)\n",
            "\tLoss: 0.0008(valid)\t|\tAcc: 71.4%(valid)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGayvMoafJEi"
      },
      "source": [
        "When the training is complete, we will load the saved_model that has the best validation accuracy  and generate model predictions on test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shIxjwGwAKnD",
        "outputId": "f33cbad4-699a-43fc-f919-01e1cb843a75"
      },
      "source": [
        "model.load_state_dict(torch.load('./saved_model'))\n",
        "predictions = eval(test_loader, test_mode=True)\n",
        "print(predictions[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 1, 1, 0, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023SQqRRab8t"
      },
      "source": [
        "# Fun with GPT-2\n",
        "\n",
        "Now, you know how to build a simple text sentiment classifer. This is just an introduction to NLP. There are many exciting things in store as you studies further. For example, you can write a cool language generator using the state-of-the-art NLP models like GPT-2. Head over to [this page](https://transformer.huggingface.co/doc/gpt2-large) to try out a language generator with GPT-2."
      ]
    }
  ]
}